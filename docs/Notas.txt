cd C:\Users\JONANNA\Desktop\airflow

docker-compose run airflow-webserver airflow users create --username admin --firstname Johanna --lastname Soler --role Admin --email johannasolermine@gmail.com --password admin123



------------------------
‚úÖ 3. Almacenamiento en SQL (Data Warehouse)
‚úîÔ∏è Lo que ya hiciste:

PostgreSQL est√° corriendo en Docker.

El DAG guarda datos en SQL (esto se puede validar si hay una tarea que cargue datos con psycopg2, sqlalchemy, o similar).

üîç ¬øQu√© debes validar para cumplir completamente?

¬øExiste un esquema claro de las tablas?

Tipos de datos, relaciones entre tablas (si hay m√°s de una).

Puedes documentarlo con un simple dibujo o exportar el schema.sql.

¬øEs un Data Warehouse?

Se espera que puedas correr consultas complejas (ejemplo: agregaciones, joins, filtros por fecha).

No es obligatorio usar star schema, pero s√≠ mostrar que la base fue pensada para an√°lisis, no solo como tabla plana.

üìå Recomendaci√≥n: crea un .sql o .md donde expliques la estructura de tu base de datos.

‚úÖ 4. Visualizaciones y Machine Learning
‚ùì Por verificar:

¬øYa hiciste las visualizaciones dentro del DAG?

Por ejemplo, un DAG que guarde gr√°ficas generadas con matplotlib en /opt/airflow/salidas.

¬øIncluiste un modelo de ML (aunque sea simple)?

Puede ser una regresi√≥n o un clustering con scikit-learn, ejecutado como una tarea del DAG.

üìå Ejemplo m√≠nimo: un task que entrena un modelo y guarda una predicci√≥n o score en la base o en un archivo .csv.

‚úÖ 5. Documentaci√≥n y Video
‚úîÔ∏è PDFs que ya subiste:

Tienes:

List DAG Run

Cluster Activity

ahora publiquemos el proyecto en github con toda la documentaci√≥n usuario djsoler, ubicaci√≥n del proyecto local C:\Users\JONANNA\Desktop airflow

Datasets

Variables

Users

DAG Grid

üîç Falta entregar:

‚úÖ Video corto mostrando:

Que el flujo se ejecuta.

Qu√© hace cada DAG.

Que Kafka-Dask-SQL est√°n integrados.

Qu√© resultado sale (gr√°fica o predicci√≥n, aunque sea simple).

‚úÖ Documentaci√≥n escrita (m√°ximo 2 p√°ginas):

Un diagrama tipo flujo del proceso (puede ser a mano).

Explicaci√≥n t√©cnica de cada fase.

Breve justificaci√≥n de elecciones (por qu√© PostgreSQL, por qu√© ese modelo, etc.).

üìå RESUMEN CHECKLIST FINAL
Requisito	¬øCumplido?	Recomendaci√≥n
DAGs orquestados (Airflow)	‚úÖ	Confirmado
Kafka funcionando	‚ùì	Verificar que est√© activo y vinculado
Dask para transformaci√≥n	‚ùì	Reemplazar Pandas por Dask si no lo hiciste
SQL como destino final	‚úÖ	Agregar esquema de la DB
Esquema DB documentado	‚ùå	Crear .sql o imagen simple explicando
Data warehouse funcional	‚úÖ parcial	Agrega ejemplo de consultas
Visualizaciones (2 m√≠n.)	‚ùì	Asegurar que est√©n dentro del DAG y guardadas
Modelo ML en DAG	‚ùì	Entrenar uno simple dentro de un task
Documentaci√≥n t√©cnica (2 p√°g)	‚ùå	Crear PDF con flujo y explicaci√≥n
Video corto de prueba	‚ùå	Grabar pantalla mostrando el flujo funcionando


-----------------------


PSQL airflow=#

docker-compose run airflow-webserver airflow users create \
    --username admin \
    --firstname Johanna \
    --lastname Soler \
    --role Admin \
    --email johannasolermine@gmail.com \
    --password admin123


proyecto_bigdata_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import pandas as pd
import matplotlib.pyplot as plt
from sqlalchemy import create_engine
from sklearn.linear_model import LinearRegression

def leer_datos_kafka():
    df = pd.read_csv('/opt/airflow/salidas/predicciones.csv')
    df.to_csv('/opt/airflow/salidas/datos_leidos.csv', index=False)

def procesar_datos_dask():
    df = pd.read_csv('/opt/airflow/salidas/datos_leidos.csv')
    df['valor'] = df['valor'] * 2
    df.to_csv('/opt/airflow/salidas/datos_procesados.csv', index=False)

def guardar_en_sql():
    df = pd.read_csv('/opt/airflow/salidas/datos_procesados.csv')
    engine = create_engine('postgresql+psycopg2://airflow:airflow@postgres:5432/airflow')
    df.to_sql('datos_guardados', engine, index=False, if_exists='replace')

def graficar():
    df = pd.read_csv('/opt/airflow/salidas/datos_procesados.csv')
    df.plot(x='fecha', y='valor', kind='line')
    plt.title('Gr√°fico de valores procesados')
    plt.xlabel('Fecha')
    plt.ylabel('Valor')
    plt.grid(True)
    plt.savefig('/opt/airflow/salidas/grafico.png')

def modelo_ml():
    df = pd.read_csv('/opt/airflow/salidas/datos_procesados.csv')
    df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce')
    df = df.dropna(subset=['fecha'])
    df['fecha'] = df['fecha'].view('int64') // 10**9
    X = df[['fecha']]
    y = df['valor']
    model = LinearRegression()
    model.fit(X, y)
    coef = model.coef_[0]
    print(f"Coeficiente de regresi√≥n: {coef}")

default_args = {
    'start_date': datetime(2024, 4, 10),
}

with DAG('proyecto_bigdata',
         schedule_interval='@once',
         catchup=False,
         default_args=default_args,
         description='Pipeline de Big Data con Airflow') as dag:

    tarea1 = PythonOperator(task_id='leer_kafka', python_callable=leer_datos_kafka)
    tarea2 = PythonOperator(task_id='procesar_datos', python_callable=procesar_datos_dask)
    tarea3 = PythonOperator(task_id='guardar_postgresql', python_callable=guardar_en_sql)
    tarea4 = PythonOperator(task_id='crear_grafico', python_callable=graficar)
    tarea5 = PythonOperator(task_id='modelo_machine_learning', python_callable=modelo_ml)

    tarea1 >> tarea2 >> tarea3 >> tarea4 >> tarea5



----------------------------------------------------
Qu√© vamos a hacer?
Agregaremos una nueva tarea Python en tu DAG, llamada por ejemplo empaquetar_resultados, que:

Buscar√° todos los archivos dentro de /opt/airflow/salidas/.

Los agrupar√° en un archivo ZIP llamado resultados_<fecha>.zip.

Guardar√° ese .zip en la misma carpeta.

üß© Paso 1: C√≥digo Python para empaquetar
Agrega este c√≥digo en tu DAG (proyecto_bigdata_dag.py):

python
Copiar
Editar
from datetime import datetime
import zipfile
import os

def empaquetar_resultados():
    carpeta_salida = "/opt/airflow/salidas"
    fecha = datetime.now().strftime("%Y%m%d_%H%M%S")
    nombre_zip = os.path.join(carpeta_salida, f"resultados_{fecha}.zip")

    with zipfile.ZipFile(nombre_zip, 'w') as zipf:
        for archivo in os.listdir(carpeta_salida):
            ruta_archivo = os.path.join(carpeta_salida, archivo)
            if os.path.isfile(ruta_archivo) and not archivo.endswith('.zip'):
                zipf.write(ruta_archivo, arcname=archivo)
üß© Paso 2: Agregar la tarea al DAG
Al final de la definici√≥n de tareas en tu DAG, a√±ade:

python
Copiar
Editar
from airflow.operators.python import PythonOperator

tarea_empaquetar = PythonOperator(
    task_id="empaquetar_resultados",
    python_callable=empaquetar_resultados,
    dag=dag,
)
üîó Paso 3: Conecta la tarea al final del flujo
En la parte donde defines el orden de ejecuci√≥n, a√±ade esta l√≠nea para que se ejecute al final:

python
Copiar
Editar
guardar_postgresql >> crear_grafico_linea >> crear_histograma >> modelo_machine_learning >> tarea_empaquetar
O si tienes algo como task1 >> task2, agrega >> tarea_empaquetar al final de la √∫ltima.

üöÄ Resultado
Una vez ejecutes el DAG, en la carpeta /opt/airflow/salidas/ (y tambi√©n en tu Windows local), ver√°s algo como:

python
Copiar
Editar
resultados_20250412_1055.zip
Y dentro estar√° todo lo que generaste üéØ

¬øTe genero el c√≥digo completo del DAG actualizado o solo el fragmento para que lo
---------------------

proyecto_bigdata

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import dask.dataframe as dd
from sqlalchemy import create_engine
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import numpy as np
import os

# Rutas
path = "/opt/airflow/salidas/"
db_url = 'postgresql://airflow:airflow@postgres:5432/airflow'

# Argumentos por defecto
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Funci√≥n 1: Ingesta (Kafka simulado prodcutor)
def leer_datos_kafka():
    df = pd.read_csv('/opt/airflow/scripts/datos.csv')  # archivo simulado
    df.to_csv(f'{path}datos_leidos.csv', index=False)

# Funci√≥n 2: Procesamiento con Dask (simula consumidor)
def procesar_datos_dask():
    df = dd.read_csv(f'{path}datos_leidos.csv')
    df['valor'] = df['valor'] * 2
    df.compute().to_csv(f'{path}datos_procesados.csv', index=False)

# Funci√≥n 3: Guardar en SQL
def guardar_en_sql():
    df = pd.read_csv(f'{path}datos_procesados.csv')
    engine = create_engine(db_url)
    df.to_sql('tabla_datos', engine, if_exists='replace', index=False)

# Funci√≥n 4a: Visualizaci√≥n 1 - L√≠nea
def graficar_linea():
    df = pd.read_csv(f'{path}datos_procesados.csv')
    plt.figure()
    plt.plot(df['valor'])
    plt.title('Gr√°fico de L√≠nea - Valor')
    plt.xlabel('√çndice')
    plt.ylabel('Valor')
    plt.savefig(f'{path}grafico_linea.png')

# Funci√≥n 4b: Visualizaci√≥n 2 - Histograma
def graficar_histograma():
    df = pd.read_csv(f'{path}datos_procesados.csv')
    plt.figure()
    df['valor'].hist()
    plt.title('Histograma de valores')
    plt.xlabel('Valor')
    plt.ylabel('Frecuencia')
    plt.savefig(f'{path}grafico_histograma.png')

# Funci√≥n 5: Modelo de ML
def modelo_ml():
    df = pd.read_csv(f'{path}datos_procesados.csv')
    X = np.arange(len(df)).reshape(-1, 1)
    y = df['valor'].values
    model = LinearRegression().fit(X, y)
    y_pred = model.predict(X)
    plt.figure()
    plt.scatter(X, y, label='Datos reales')
    plt.plot(X, y_pred, color='red', label='Predicci√≥n')
    plt.title('Regresi√≥n lineal')
    plt.legend()
    plt.savefig(f'{path}modelo_ml.png')

# DAG
with DAG(
    dag_id='proyecto_bigdata',
    default_args=default_args,
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    tags=['bigdata']
) as dag:

    t1 = PythonOperator(task_id='leer_kafka', python_callable=leer_datos_kafka)
    t2 = PythonOperator(task_id='procesar_datos_dask', python_callable=procesar_datos_dask)
    t3 = PythonOperator(task_id='guardar_postgresql', python_callable=guardar_en_sql)
    t4a = PythonOperator(task_id='crear_grafico_linea', python_callable=graficar_linea)
    t4b = PythonOperator(task_id='crear_histograma', python_callable=graficar_histograma)
    t5 = PythonOperator(task_id='modelo_machine_learning', python_callable=modelo_ml)

    # Dependencias
    t1 >> t2 >> t3 >> [t4a, t4b] >> t5

--------------------
Por favor, gu√≠ame para la construcci√≥n del video (grabando el paso a paso),


from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import dask.dataframe as dd
from sqlalchemy import create_engine
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import numpy as np
import os

# Rutas
path = "/opt/airflow/salidas/"
db_url = 'postgresql://airflow:airflow@postgres:5432/airflow'

# Argumentos por defecto
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# 1. Ingesta de datos (Kafka simulado)
# 1. Ingesta de datos (Kafka simulado)
def leer_datos_kafka():
    import pandas as pd
    import os

    input_path = '/opt/airflow/salidas/datos_procesados.csv'
    output_path = '/opt/airflow/salidas/datos_leidos.csv'

    if not os.path.exists(input_path):
        raise FileNotFoundError(f"Archivo no encontrado: {input_path}")

    df = pd.read_csv(input_path)
    df.to_csv(output_path, index=False)
    print(f'Datos simulados como Kafka le√≠dos desde {input_path} y guardados en {output_path}')

# 2. Procesamiento con Dask
def procesar_datos_dask():
    df = dd.read_csv(f'{path}datos_leidos.csv')
    df['valor'] = df['valor'] * 2
    df.compute().to_csv(f'{path}datos_procesados.csv', index=False)

# 3. Guardar en SQL
def guardar_en_sql():
    df = pd.read_csv(f'{path}datos_procesados.csv')
    engine = create_engine(db_url)
    df.to_sql('tabla_datos', engine, if_exists='replace', index=False)

# 4a. Gr√°fico de l√≠nea
def graficar_linea():
    df = pd.read_csv(f'{path}datos_procesados.csv')
    plt.figure()
    plt.plot(df['valor'])
    plt.title('Gr√°fico de L√≠nea - Valor')
    plt.xlabel('√çndice')
    plt.ylabel('Valor')
    plt.savefig(f'{path}grafico_linea.png')

# 4b. Funci√≥n para graficar outliers
def crear_grafico_outliers():
    import os
    os.system("python /opt/airflow/scripts/graficar_outliers.py")

# 4c. Histograma
def graficar_histograma():
    df = pd.read_csv(f'{path}datos_procesados.csv')
    plt.figure()
    df['valor'].hist()
    plt.title('Histograma de valores')
    plt.xlabel('Valor')
    plt.ylabel('Frecuencia')
    plt.savefig(f'{path}grafico_histograma.png')

# 5. Modelo simple en l√≠nea (visual)
def modelo_ml():
    df = pd.read_csv(f'{path}datos_procesados.csv')
    X = np.arange(len(df)).reshape(-1, 1)
    y = df['valor'].values
    model = LinearRegression().fit(X, y)
    y_pred = model.predict(X)
    plt.figure()
    plt.scatter(X, y, label='Datos reales')
    plt.plot(X, y_pred, color='red', label='Predicci√≥n')
    plt.title('Regresi√≥n lineal')
    plt.legend()
    plt.savefig(f'{path}modelo_ml.png')

# 6. Nuevo task: ejecutar script externo para entrenar modelo y guardar predicciones
def entrenar_modelo_ml():
    script_path = "/opt/airflow/scripts/modelo_ml.py"
    os.system(f"python {script_path}")

# DAG
with DAG(
    dag_id='proyecto_bigdata',
    default_args=default_args,
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    tags=['bigdata'],
    description='Flujo de datos del IDEAM - Precipitaci√≥n mensual 2020-2025. Departamento del Meta, estaci√≥n SENA (Villavicencio).',
    doc_md="""
    Este DAG procesa datos de precipitaci√≥n mensual total del IDEAM, correspondientes al per√≠odo 2020‚Äì2025, para el departamento del **Meta**,
    ciudad **Villavicencio**, estaci√≥n de medici√≥n **SENA**. Se realiza limpieza, an√°lisis visual y modelado de predicci√≥n.
    """
) as dag:

    t1 = PythonOperator(task_id='leer_kafka', python_callable=leer_datos_kafka)
    t2 = PythonOperator(task_id='procesar_datos_dask', python_callable=procesar_datos_dask)
    t3 = PythonOperator(task_id='guardar_postgresql', python_callable=guardar_en_sql)
    t4a = PythonOperator(task_id='crear_grafico_linea', python_callable=graficar_linea)
    t4b = PythonOperator(task_id='crear_grafico_outliers', python_callable=crear_grafico_outliers)
    t4c = PythonOperator(task_id='crear_histograma', python_callable=graficar_histograma)
    t5 = PythonOperator(task_id='modelo_machine_learning', python_callable=modelo_ml)
    t6 = PythonOperator(task_id='entrenar_modelo_ml', python_callable=entrenar_modelo_ml)

    # Dependencias
    t1 >> t2 >> t3 >> [t4a, t4b, t4c] >> t5 >> t6

----------------------
DOCKER

version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  airflow-webserver:
    image: apache/airflow:2.8.1
    build:                         # üëà A√ëADIR ESTA SECCI√ìN
      context: .
      dockerfile: Dockerfile
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./dags:/opt/airflow/dags
      
      - ./salidas:/opt/airflow/salidas
    ports:
      - "8080:8080"
    command: >
      bash -c "airflow db migrate && airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin && airflow webserver"

  airflow-scheduler:
    image: apache/airflow:2.8.1
    build:                         # üëà A√ëADIR ESTA SECCI√ìN TAMBI√âN AQU√ç
      context: .
      dockerfile: Dockerfile
    depends_on:
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
    command: airflow scheduler

volumes:
  postgres-db-volume:

------------------

